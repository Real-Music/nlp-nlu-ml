{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for NLP: Word Embeddings for Deep Learning in Keras\n",
    "\n",
    "In this article, we will study word embeddings for NLP tasks that involve deep learning. We will see how word embeddings can be used to perform simple classification task using deep neural network in Python's Keras Library.\n",
    "\n",
    "## Problems with One-Hot Encoded Feature Vector Approaches\n",
    "\n",
    "A potential drawback with one-hot encoded feature vector approaches such as N-Grams, bag of words and TF-IDF approach is that the feature vector for each document can be huge. For instance, if you have a half million unique words in your corpus and you want to represent a sentence that contains 10 words, your feature vector will be a half million dimensional one-hot encoded vector where only 10 indexes will have 1. This is a wastage of space and increases algorithm complexity exponentially resulting in the curse of dimentionality.\n",
    "\n",
    "## Word Embeddings\n",
    "In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vector. Word embeddings techniques such as GloVe and Word2Vec have proven to be extremely efficient for converting words into corresponding dense vectors. The vector size is small and none of the indexes in the vector is actually empty.\n",
    "\n",
    "## Implementing Word Embeddings with Keras Sequential Models\n",
    "The Keras library is one of the most famous and commonly used deep learning libraries for Python that is built on top of TensorFlow.\n",
    "\n",
    "Keras support two types of APIs: Sequential and Functional. In this section we will see how word embeddings are used with Keras Sequential API. In the next section, I will explain how to implement the same model via the Keras functional API.\n",
    "\n",
    "To implement word embeddings, the Keras library contains a layer called Embedding(). The embedding layer is implemented in the form of a class in Keras and is normally used as a first layer in the sequential model for NLP tasks.\n",
    "\n",
    "The embedding layer can be used to peform three tasks in Keras:\n",
    "\n",
    "* It can be used to learn word embeddings and save the resulting model\n",
    "* It can be used to learn the word embeddings in addition to performing the NLP tasks such as text classification, sentiment analysis, etc.\n",
    "* It can be used to load pretrained word embeddings and use them in a new model\n",
    "\n",
    "In this article, we will see the second and third use-case of the Embedding layer. The first use-case is a subset of the second use-case.\n",
    "\n",
    "Let's see how the embedding layer looks:\n",
    "\n",
    "`embedding_layer = Embedding(200, 32, input_length=50)`\n",
    "\n",
    "The first parameter in the embeddig layer is the size of the vocabulary or the total number of unique words in a corpus. The second parameter is the number of the dimensions for each word vector. For instance, if you want each word vector to have 32 dimensions, you will specify 32 as the second parameter. And finally, the third parameter is the length of the input sentence.\n",
    "\n",
    "The output of the word embedding is a 2D vector where words are represented in rows, whereas their corresponding dimensions are presented in columns. Finally, if you wish to directly connect your word embedding layer with a densely connected layer, you first have to flatten your 2D word embeddings into 1D. These concepts will become more understandable once we see word embedding in action.\n",
    "\n",
    "## Custom Word Embeddings\n",
    "\n",
    "As I said earlier, Keras can be used to either learn custom words embedding or it can be used to load pretrained word embeddings. In this section, we will see how the Keras Embedding Layer can be used to learn custom word embeddings.\n",
    "\n",
    "We will perform simple text classification tasks that will use word embeddings. Execute the following script to download the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Dense, Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our dataset. We will be using a very simple custom dataset that will contain reviews above movies. The following script creates our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our corpus has 8 positive reviews and 8 negative reviews. The next step is to create label set for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first 8 items in the sentiment array contain 1, which correspond to positive sentiment. The last 8 items are zero that correspond to negative sentiment.\n",
    "\n",
    "Earlier we said that the first parameter to the Embedding() layer is the vocabulary, or number of unique words in the corpus. Let's first find the total number of words in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for sent in corpus:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    all_words = all_words + tokenize_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we simply iterate through each sentence in our corpus and then tokenize the sentence into words. Next, we iterate through the list of all the words and append the words into the all_words list. Once you execute the above script, you should see all the words in the all_words dictionary. However, we do not want the duplicate words.\n",
    "\n",
    "We can retrieve all the unique words from a list by passing the list into the set function, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(all_words)\n",
    "print(len(unique_words))\n",
    "vocab_length = len(unique_words) + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output you will see \"45\", which is the number of unique words in our corpus. We will add a buffer of 5 to our vocabulary size and will set the value of vocab_length to 50.\n",
    "\n",
    "The Embedding layer expects the words to be in numeric form. Therefore, we need to convert the sentences in our corpus to numbers. One way to convert text to numbers is by using the one_hot function from the keras.preprocessing.text library. The function takes sentence and the total length of the vocabulary and returns the sentence in numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38, 32, 24, 41, 48], [39, 38, 31, 39, 48, 41, 13], [25, 30, 34, 13, 32, 18], [30, 44], [28, 47, 18, 38, 48, 41, 13], [19, 13, 39, 8], [4, 20, 32, 13, 35, 48], [13, 32, 13, 6, 48], [11, 12], [43, 19, 32], [6, 47], [13, 31, 23, 49], [48, 40, 25, 41, 39, 48], [39, 48, 31, 11], [48, 27, 25, 48], [39, 12, 32, 6]]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences = [ one_hot(sent, vocab_length) for sent in corpus]\n",
    "print(embedded_sentences )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we convert all the sentences in our corpus to their numeric form and display them on the console. The output looks like this:\n",
    "\n",
    "You can see that our first sentence contained five words, therefore we have five integers in the first list item. Also, notice that the last word of the first sentence was \"movie\" in the first list item, and we have digit 48 at the fifth place of the resulting 2D array, which means that \"movie\" has been encoded as 48 and so on.\n",
    "\n",
    "The embedding layer expects sentences to be of equal size. However, our encoded sentences are of different sizes. One way to make all the sentences of uniform size is to increase the lenght of all the sentences and make it equal to the length of the largest sentence. Let's first find the largest sentence in our corpus and then we will increase the length of all the sentences to the length of the largest sentence. To do so, execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sentence above, we use a lambda expression to find the length of all the sentences. We then use the max function to return the longest sentence. Finally the longest sentence is tokenized into words and the number of words are counted using the len function.\n",
    "\n",
    "Next to make all the sentences of equal size, we will add zeros to the empty indexes that will be created as a result of increasing the sentence length. To append the zeros at the end of the sentencses, we can use the pad_sequences method. The `first parameter is the list of encoded sentences of unequal sizes, the second parameter is the size of the longest sentence or the padding index, while the last parameter is padding where you specify post to add padding at the end of sentences.`\n",
    "\n",
    "Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38 32 24 41 48  0  0]\n",
      " [39 38 31 39 48 41 13]\n",
      " [25 30 34 13 32 18  0]\n",
      " [30 44  0  0  0  0  0]\n",
      " [28 47 18 38 48 41 13]\n",
      " [19 13 39  8  0  0  0]\n",
      " [ 4 20 32 13 35 48  0]\n",
      " [13 32 13  6 48  0  0]\n",
      " [11 12  0  0  0  0  0]\n",
      " [43 19 32  0  0  0  0]\n",
      " [ 6 47  0  0  0  0  0]\n",
      " [13 31 23 49  0  0  0]\n",
      " [48 40 25 41 39 48  0]\n",
      " [39 48 31 11  0  0  0]\n",
      " [48 27 25 48  0  0  0]\n",
      " [39 12 32  6  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see zeros at the end of the padded sentences.\n",
    "\n",
    "Now we have everything that we need to create a sentiment classification model using word embeddings.\n",
    "\n",
    "We will create a very simple text classification model with an embedding layer and no hidden layers. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we create a Sequential model and add the Embedding layer as the first layer to the model. The length of the vocabulary is specified by the vocab_length parameter. The dimension of each word vector will be 20 and the input_length will be the length of the longest sentence, which is 7. Next, the Embedding layer is flattened so that it can be directly used with the densely connected layer. Since it is a binary classification problem, we use the sigmoid function as the loss function at the dense layer.\n",
    "\n",
    "Next, we will compile the model and print the summary of our model, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 20)             1000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 141       \n",
      "=================================================================\n",
      "Total params: 1,141\n",
      "Trainable params: 1,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first layer has 1000 trainable parameters. This is because our vocabulary size is 50 and each word will be presented as a 20 dimensional vector. Hence the total number of trainable parameters will be 1000. Similarly, the output from the embedding layer will be a sentence with 7 words where each word is represented by a 20 dimensional vector. However, when the 2D output is flattened, we get a 140 dimensional vector (7 x 20). The flattened vector is directly connected to the dense layer that contains 1 neuran.\n",
    "\n",
    "Now let's train the model on our data using the fit method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6945 - acc: 0.4375\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 725us/step - loss: 0.6912 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.6880 - acc: 0.6250\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 932us/step - loss: 0.6847 - acc: 0.7500\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 709us/step - loss: 0.6815 - acc: 0.7500\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 811us/step - loss: 0.6782 - acc: 0.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 889us/step - loss: 0.6750 - acc: 0.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6718 - acc: 0.8750\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 890us/step - loss: 0.6686 - acc: 0.8750\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6653 - acc: 0.9375\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 969us/step - loss: 0.6621 - acc: 0.9375\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 896us/step - loss: 0.6588 - acc: 0.9375\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 800us/step - loss: 0.6556 - acc: 0.9375\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6523 - acc: 0.9375\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 780us/step - loss: 0.6490 - acc: 0.9375\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 778us/step - loss: 0.6457 - acc: 0.9375\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 852us/step - loss: 0.6423 - acc: 0.9375\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 696us/step - loss: 0.6389 - acc: 0.9375\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 970us/step - loss: 0.6356 - acc: 0.9375\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 738us/step - loss: 0.6321 - acc: 0.9375\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 985us/step - loss: 0.6287 - acc: 0.9375\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6252 - acc: 0.9375\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 844us/step - loss: 0.6217 - acc: 0.9375\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6182 - acc: 0.9375\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 854us/step - loss: 0.6146 - acc: 0.9375\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 792us/step - loss: 0.6110 - acc: 0.9375\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 711us/step - loss: 0.6073 - acc: 0.9375\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 739us/step - loss: 0.6037 - acc: 0.9375\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 690us/step - loss: 0.6000 - acc: 0.9375\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 737us/step - loss: 0.5962 - acc: 0.9375\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 874us/step - loss: 0.5924 - acc: 0.9375\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 770us/step - loss: 0.5886 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 689us/step - loss: 0.5848 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 791us/step - loss: 0.5809 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 735us/step - loss: 0.5769 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 741us/step - loss: 0.5730 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5690 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5649 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5609 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 787us/step - loss: 0.5568 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 650us/step - loss: 0.5526 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 628us/step - loss: 0.5484 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 756us/step - loss: 0.5442 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 671us/step - loss: 0.5400 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5357 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 769us/step - loss: 0.5314 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 702us/step - loss: 0.5270 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 723us/step - loss: 0.5227 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 789us/step - loss: 0.5183 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 982us/step - loss: 0.5138 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5094 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 947us/step - loss: 0.5049 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 655us/step - loss: 0.5004 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 837us/step - loss: 0.4959 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 733us/step - loss: 0.4913 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 775us/step - loss: 0.4867 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 716us/step - loss: 0.4821 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 793us/step - loss: 0.4775 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 923us/step - loss: 0.4729 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4683 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 964us/step - loss: 0.4636 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 978us/step - loss: 0.4589 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 758us/step - loss: 0.4543 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 716us/step - loss: 0.4496 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4449 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 721us/step - loss: 0.4402 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 715us/step - loss: 0.4355 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 950us/step - loss: 0.4308 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 870us/step - loss: 0.4261 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 806us/step - loss: 0.4214 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 894us/step - loss: 0.4167 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 809us/step - loss: 0.4120 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4073 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4026 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 809us/step - loss: 0.3979 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 841us/step - loss: 0.3933 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 809us/step - loss: 0.3886 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3840 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 813us/step - loss: 0.3794 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 877us/step - loss: 0.3748 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 759us/step - loss: 0.3702 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 756us/step - loss: 0.3656 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 889us/step - loss: 0.3610 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 935us/step - loss: 0.3565 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 831us/step - loss: 0.3520 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 835us/step - loss: 0.3475 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 948us/step - loss: 0.3430 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 877us/step - loss: 0.3386 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 863us/step - loss: 0.3342 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 687us/step - loss: 0.3298 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 708us/step - loss: 0.3254 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 848us/step - loss: 0.3211 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3168 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 741us/step - loss: 0.3125 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 689us/step - loss: 0.3083 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 921us/step - loss: 0.3041 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 838us/step - loss: 0.2999 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 743us/step - loss: 0.2958 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 693us/step - loss: 0.2917 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 757us/step - loss: 0.2877 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb072710a90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be trained for 100 epochs.\n",
    "\n",
    "We will train and test the model using the same corpus. Execute the following script to evaluate the model performance on our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output, you will see that model accuracy is 1.00 i.e. 100 percent.\n",
    "\n",
    "Note: In real world applications, train and test sets should be different. We will see an example of that when we perform text classification on some real world data in an upcoming article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ray': conda)",
   "language": "python",
   "name": "python37664bitrayconda3b8e9fc2afed4f51b4e648b9e2d2c0cd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

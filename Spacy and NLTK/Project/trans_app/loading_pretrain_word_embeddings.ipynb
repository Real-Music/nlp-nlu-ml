{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Word Embeddings\n",
    "\n",
    "In the previous section we trained custom word embeddings. However, we can also use pretrained word embeddings. \n",
    "<a href=\"https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\"> Tuto Link</a>\n",
    "\n",
    "Several types of pretrained word embeddings exist, however we will be using the GloVe word embeddings from Stanford NLP since it is the most famous one and commonly used. The word embeddings can be downloaded from <a href=\"https://nlp.stanford.edu/projects/glove/\">this link.</a>\n",
    "\n",
    "The smallest file is named \"Glove.6B.zip\". The size of the file is 822 MB. The file contains 50, 100, 200, and 300 dimensional word vectors for 400k words. We will be using the 100 dimensional vector.\n",
    "\n",
    "The process is quite similar. First we have to import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to create our corpus followed by the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic'\n",
    "]\n",
    "sentiments = array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section, we used one_hot function to convert text to vectors. Another approach is to use Tokenizer function from keras.preprocessing.text library.\n",
    "\n",
    "You simply have to pass your corpus to the Tokenizer's fit_on_text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the number of unique words in the text, you can simply count the length of word_index dictionary of the word_tokenizer object. Remember to add 1 with the vocabulary size. This is to store the dimensions for the words for which no pretrained word embeddings exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to convert sentences to their numeric counterpart, call the texts_to_sequences function and pass it the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 3, 15, 16, 1], [4, 17, 6, 9, 5, 7, 2], [18, 19, 20, 2, 3, 21], [22, 23], [24, 25, 26, 27, 5, 7, 2], [28, 8, 9, 29], [30, 31, 32, 8, 33, 1], [2, 3, 8, 34, 1], [10, 11], [35, 36, 37], [12, 38], [2, 6, 39, 40], [5, 41, 13, 7, 4, 1], [4, 1, 6, 10], [5, 42, 13, 43], [4, 11, 3, 12]]\n"
     ]
    }
   ],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
    "print(embedded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the number of words in the longest sentence and then to apply padding to the sentences having shorter lengths than the length of the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  3 15 16  1  0  0]\n",
      " [ 4 17  6  9  5  7  2]\n",
      " [18 19 20  2  3 21  0]\n",
      " [22 23  0  0  0  0  0]\n",
      " [24 25 26 27  5  7  2]\n",
      " [28  8  9 29  0  0  0]\n",
      " [30 31 32  8 33  1  0]\n",
      " [ 2  3  8 34  1  0  0]\n",
      " [10 11  0  0  0  0  0]\n",
      " [35 36 37  0  0  0  0]\n",
      " [12 38  0  0  0  0  0]\n",
      " [ 2  6 39 40  0  0  0]\n",
      " [ 5 41 13  7  4  1  0]\n",
      " [ 4  1  6 10  0  0  0]\n",
      " [ 5 42 13 43  0  0  0]\n",
      " [ 4 11  3 12  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted our sentences into padded sequence of numbers. The next step is to load the GloVe word embeddings and then create our embedding matrix that contains the words in our corpus and their corresponding values from GloVe embeddings. Run the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('./data/archive/glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, in addition to loading the GloVe embeddings, we also imported a few libraries. We will see the use of these libraries in the upcoming section. Here notice that we loaded glove.6B.100d.txt file. This file contains 100 dimensional word embeddings. We also created an empty dictionary that will store our word embeddings.\n",
    "\n",
    "If you open the file, you will see a word at the beginning of each line followed by set of 100 numbers. The numbers form the 100 dimensional vector for the word at the begining of each line.\n",
    "\n",
    "We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, in the form of an array. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype=\"float32\")\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "    \n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words.\n",
    "\n",
    "The dictionary embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words.\n",
    "\n",
    "We want the word embeddings for only those words that are present in our corpus. We will create a two dimensional numpy array of 44 (size of vocabulary) rows and 100 columns. The array will initially contain zeros. The array will be named as embedding_matrix\n",
    "\n",
    "Next, we will iterate through each word in our corpus by traversing the word_tokenizer.word_index dictionary that contains our words and their corresponding index.\n",
    "\n",
    "Each word will be passed as key to the embedding_dictionary to retrieve the corresponding 100 dimensional vector for the word. The 100 dimensional vector will then be stored at the corresponding index of the word in the embedding_matrix. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding_matrix now contains pretrained word embeddings for the words in our corpus.\n",
    "\n",
    "Now we are ready to create our sequential model. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script remains the same, except for the embedding layer. Here in the embedding layer, the first parameter is the size of the vacabulary. The second parameter is the vector dimension of the output vector. Since we are using pretrained word embeddings that contain 100 dimensional vector, we set the vector dimension to 100.\n",
    "\n",
    "Another very important attribute of the Embedding() layer that we did not use in the last section is weights. You can pass your pretrained embedding matrix as default weights to the weights parameter. And since we are not training the embedding layer, the trainable attribute has been set to False.\n",
    "\n",
    "Let's compile our model and see the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 7, 100)            4400      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 5,101\n",
      "Trainable params: 701\n",
      "Non-trainable params: 4,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again using adam as the optizer to minimize the loss. The loss function being used is binary_crossentropy. And we want to see the results in the form of accuracy so acc has been passed as the value for the metrics attribute.\n",
    "\n",
    "You can see that since we have 44 words in our vocabulary and each word will be represented as a 100 dimensional vector, the number of parameters for the embedding layer will be 44 x 100 = 4400. The output from the embedding layer will be a 2D vector with 7 rows (1 for each word in the sentence) and 100 columns. The output from the embedding layer will be flattened so that it can be used with the dense layer. Finally the dense layer is used to make predictions.\n",
    "\n",
    "Execute the following script to train the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6514 - acc: 0.5625\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 664us/step - loss: 0.6252 - acc: 0.5625\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 816us/step - loss: 0.6014 - acc: 0.6250\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 843us/step - loss: 0.5797 - acc: 0.6875\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.5598 - acc: 0.8125\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 739us/step - loss: 0.5412 - acc: 0.8750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 688us/step - loss: 0.5239 - acc: 0.8750\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5074 - acc: 0.9375\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 760us/step - loss: 0.4916 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4763 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 813us/step - loss: 0.4616 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 928us/step - loss: 0.4472 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 938us/step - loss: 0.4333 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 739us/step - loss: 0.4197 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4065 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 895us/step - loss: 0.3937 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 769us/step - loss: 0.3813 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3694 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 836us/step - loss: 0.3579 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 786us/step - loss: 0.3469 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 962us/step - loss: 0.3363 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 966us/step - loss: 0.3262 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3165 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 670us/step - loss: 0.3071 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 899us/step - loss: 0.2982 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2896 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 835us/step - loss: 0.2813 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 635us/step - loss: 0.2734 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 571us/step - loss: 0.2657 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 641us/step - loss: 0.2584 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 668us/step - loss: 0.2513 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 613us/step - loss: 0.2444 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 599us/step - loss: 0.2379 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 750us/step - loss: 0.2315 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 636us/step - loss: 0.2254 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 866us/step - loss: 0.2195 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 752us/step - loss: 0.2138 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 695us/step - loss: 0.2084 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 692us/step - loss: 0.2032 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 760us/step - loss: 0.1981 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 746us/step - loss: 0.1932 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1886 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 729us/step - loss: 0.1841 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 709us/step - loss: 0.1797 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 709us/step - loss: 0.1755 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 649us/step - loss: 0.1715 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1676 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 829us/step - loss: 0.1638 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 892us/step - loss: 0.1602 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 861us/step - loss: 0.1567 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1533 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 757us/step - loss: 0.1500 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 726us/step - loss: 0.1468 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1438 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1408 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 739us/step - loss: 0.1379 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 708us/step - loss: 0.1351 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 904us/step - loss: 0.1325 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 661us/step - loss: 0.1299 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 828us/step - loss: 0.1273 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1249 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 772us/step - loss: 0.1225 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 858us/step - loss: 0.1202 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 710us/step - loss: 0.1180 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 778us/step - loss: 0.1159 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1138 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1117 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 845us/step - loss: 0.1097 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1078 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 702us/step - loss: 0.1060 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 731us/step - loss: 0.1042 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 846us/step - loss: 0.1024 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1007 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0990 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 865us/step - loss: 0.0974 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 734us/step - loss: 0.0958 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 792us/step - loss: 0.0943 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 859us/step - loss: 0.0928 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 808us/step - loss: 0.0913 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 849us/step - loss: 0.0899 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 740us/step - loss: 0.0885 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 768us/step - loss: 0.0872 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 841us/step - loss: 0.0858 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 825us/step - loss: 0.0846 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0833 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 754us/step - loss: 0.0821 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0809 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 734us/step - loss: 0.0797 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0786 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 963us/step - loss: 0.0775 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0764 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 788us/step - loss: 0.0753 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 789us/step - loss: 0.0743 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0733 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 767us/step - loss: 0.0723 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 675us/step - loss: 0.0713 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 825us/step - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 683us/step - loss: 0.0695 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 745us/step - loss: 0.0686 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 663us/step - loss: 0.0677 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3bac494290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the algorithm is trained, run the following script to evaluate the peformance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings with Keras Functional API\n",
    "In the last section, we saw how word embeddings can be used with the Keras sequential API. While the sequential API is a good starting point for beginners, as it allows you to quickly create deep learning models, it is extremely important to know how Keras Functional API works. Most of the advanced deep learning models involving multiple inputs and outputs use the Functional API.\n",
    "\n",
    "In this section, we will see how we can implement embedding layer with Keras Functional API.\n",
    "\n",
    "The rest of the script remains similar as it was in the last section. The only change will be in the development of a deep learning model. Let's implement the same deep learning model as we implemented in the last section with Keras Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "deep_inputs = Input(shape=(length_long_sentence,))\n",
    "embedding = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=length_long_sentence, trainable=False)(deep_inputs) # line A\n",
    "flatten = Flatten()(embedding)\n",
    "hidden = Dense(1, activation='sigmoid')(flatten)\n",
    "model = Model(inputs=deep_inputs, outputs=hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Keras Functional API, you have to define the input layer separately before the embedding layer. In the input, layer you have to simply pass the length of input vector. To specify that previous layer as input to the next layer, the previous layer is passed as a parameter inside the parenthesis, at the end of the next layer.\n",
    "\n",
    "For instance, in the above script, you can see that deep_inputs is passed as parameter at the end of the embedding layer. Similarly, embedding is passed as input at the end of the Flatten() layer and so on.\n",
    "\n",
    "Finally, in the Model(), you have to pass the input layer, and the final output layer.\n",
    "\n",
    "Let's now compile the model and take a look at the summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 7)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 7, 100)            4400      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 700)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 5,101\n",
      "Trainable params: 701\n",
      "Non-trainable params: 4,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model summary, you can see the input layer as a separate layer before the embedding layer. The rest of the model remains the same.\n",
    "\n",
    "Finally, the process to fit and evaluate the model is same as the one used in Sequential API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 912us/step - loss: 0.0717 - acc: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 818us/step - loss: 0.0708 - acc: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0699 - acc: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 835us/step - loss: 0.0690 - acc: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 863us/step - loss: 0.0681 - acc: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 713us/step - loss: 0.0673 - acc: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 632us/step - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0656 - acc: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 839us/step - loss: 0.0648 - acc: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 895us/step - loss: 0.0640 - acc: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 910us/step - loss: 0.0633 - acc: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0625 - acc: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 874us/step - loss: 0.0618 - acc: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0610 - acc: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0603 - acc: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0596 - acc: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 710us/step - loss: 0.0590 - acc: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 783us/step - loss: 0.0583 - acc: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 699us/step - loss: 0.0576 - acc: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0570 - acc: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 852us/step - loss: 0.0563 - acc: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 819us/step - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 898us/step - loss: 0.0551 - acc: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0545 - acc: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 868us/step - loss: 0.0539 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0533 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 923us/step - loss: 0.0528 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 637us/step - loss: 0.0522 - acc: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 793us/step - loss: 0.0517 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 677us/step - loss: 0.0511 - acc: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0506 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 724us/step - loss: 0.0501 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 869us/step - loss: 0.0496 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 777us/step - loss: 0.0491 - acc: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 953us/step - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 797us/step - loss: 0.0481 - acc: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 799us/step - loss: 0.0476 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 877us/step - loss: 0.0471 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 919us/step - loss: 0.0467 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 839us/step - loss: 0.0462 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 863us/step - loss: 0.0458 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 771us/step - loss: 0.0453 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 855us/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 944us/step - loss: 0.0444 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 857us/step - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 763us/step - loss: 0.0436 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 828us/step - loss: 0.0432 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 935us/step - loss: 0.0428 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 780us/step - loss: 0.0424 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 938us/step - loss: 0.0420 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 689us/step - loss: 0.0416 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0412 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 758us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0405 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 807us/step - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0398 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0391 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 875us/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 761us/step - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 751us/step - loss: 0.0381 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 930us/step - loss: 0.0378 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 817us/step - loss: 0.0371 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 684us/step - loss: 0.0368 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 880us/step - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 813us/step - loss: 0.0362 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 659us/step - loss: 0.0359 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 651us/step - loss: 0.0356 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 795us/step - loss: 0.0353 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 804us/step - loss: 0.0350 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 867us/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 920us/step - loss: 0.0344 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 715us/step - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 712us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 863us/step - loss: 0.0336 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 783us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 782us/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 766us/step - loss: 0.0323 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 815us/step - loss: 0.0321 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 751us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 915us/step - loss: 0.0313 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 765us/step - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 672us/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 840us/step - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 695us/step - loss: 0.0304 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 702us/step - loss: 0.0302 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 744us/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 945us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 696us/step - loss: 0.0293 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 844us/step - loss: 0.0291 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 727us/step - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 842us/step - loss: 0.0287 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 721us/step - loss: 0.0283 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 710us/step - loss: 0.0281 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3acc0d3b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "To use text data as input to the deep learning model, we need to convert text to numbers. However unlike machine learning models, passing sparse vector of huge sizes can greately affect deep learning models. Therefore, we need to convert our text to small dense vectors. Word embeddings help us convert text to dense vectors.\n",
    "\n",
    "In this article we saw how word embeddings can be implemented with Keras deep learning library. We implemented the custom word embeddings as well as used pretrained word embedddings to solve simple classification task. Finally, we also saw how to implement word embeddings with Keras Functional API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ray': conda)",
   "language": "python",
   "name": "python37664bitrayconda3b8e9fc2afed4f51b4e648b9e2d2c0cd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJmirwqHCBXP"
   },
   "source": [
    "# Attention is All You Need\n",
    "\n",
    "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. All images in this notebook will be taken from the Transformer paper. For more information about the Transformer, [see](https://www.mihaileric.com/posts/transformers-attention-in-disguise/) [these](https://jalammar.github.io/illustrated-transformer/) [three](http://nlp.seas.harvard.edu/2018/04/03/attention.html) articles.\n",
    "\n",
    "<img src=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/transformer1.png?raw=1\"/>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization. \n",
    "\n",
    "As of January 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future. \n",
    "\n",
    "The most popular Transformer variant is [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) and pre-trained versions of BERT are commonly used to replace the embedding layers - if not more - in NLP models. \n",
    "\n",
    "A common library used when dealing with pre-trained transformers is the [Transformers](https://huggingface.co/transformers/) library, see [here](https://huggingface.co/transformers/pretrained_models.html) for a list of all pre-trained models available.\n",
    "\n",
    "The differences between the implementation in this notebook and the paper are:\n",
    "- we use a learned positional encoding instead of a static one\n",
    "- we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
    "- we do not use label smoothing\n",
    "\n",
    "We make all of these changes as they closely follow BERT's set-up and the majority of Transformer variants use a similar set-up.\n",
    "\n",
    "#### Dataset\n",
    "* src -> English\n",
    "* trg -> French\n",
    "\n",
    "#### Embedding\n",
    "*  input_dim -> vocab_size is the number of words in your train, val and test set\n",
    "* hid_dim -> embedding_size: each word is map to some dimisional space\n",
    "\n",
    "* n_layer -> num_layers\n",
    "* n_heads -> num_heads\n",
    "\n",
    "* pf_dim -> forward_expansion\n",
    "\n",
    "#### Perplexity\n",
    "The lower the perplexity, the more likely the sentence is to sound natural to human ears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13418,
     "status": "ok",
     "timestamp": 1609925532466,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "AIOj6QrgCBXZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "spacy_fr = spacy.load('fr')\n",
    "spacy_en = spacy.load('en')\n",
    "# spacy_de = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1609925533696,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "bAW_s8PlCH3z"
   },
   "outputs": [],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "# !python -m spacy download fr\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EDXCafECBXa"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1609925533697,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "p90ItHI2CBXb"
   },
   "outputs": [],
   "source": [
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def print_terminal(arg: str):\n",
    "    print(f\"========== {arg} ==========\\n\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print_terminal(f'The model has {params:,} trainable parameters')\n",
    "    \n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        \n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def set_seed(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    print_terminal(f\"Default seed set: {seed}\")\n",
    "    \n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg #trg = [batch size, trg len]\n",
    " \n",
    "            output, _ = model(src, trg[:,:-1]) #output = [batch size, trg len - 1, output dim]\n",
    "        \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim) #output = [batch size * trg len - 1, output dim]\n",
    "            trg = trg[:,1:].contiguous().view(-1) #trg = [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnTfwZOGCBXc"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1609925534042,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "esxG7FX_CBXc"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        tokens = tokenize_fr(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    \n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "        \n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    for i in range(max_len):\n",
    "        \n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "            \n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:], attention\n",
    "\n",
    "def display_attention(sentence, translation, attention, num_heads=8, num_rows=4, num_cols=2):\n",
    "    assert num_rows * num_cols == num_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    for i in range(num_heads):\n",
    "        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, i+1)\n",
    "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "        \n",
    "        cax = ax.matshow(_attention, cmap='bone')\n",
    "        ax.tick_params(labelsize=12)\n",
    "        \n",
    "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def model_test(dataset, src_field, trg_field, model, device, max_len=50):\n",
    "    example_idx = random.randint(1, len(dataset.examples))\n",
    "    \n",
    "    src = vars(dataset.examples[example_idx])['src']\n",
    "    trg = vars(dataset.examples[example_idx])['trg']\n",
    "    \n",
    "    src_ = ' '.join([a for a in src])\n",
    "    trg_ = ' '.join([a for a in trg])\n",
    "\n",
    "    print_terminal(f'src = {src_}') \n",
    "    print_terminal(f'trg = {trg_}')\n",
    "\n",
    "    translation, attention = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "    sen = ' '.join([a for a in translation])\n",
    "    print_terminal(f'predicted trg = {sen}')\n",
    "    print_terminal(len(translation))\n",
    "\n",
    "#     display_attention(src, translation, attention)\n",
    "    \n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len=50):\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6wx9Z_xCBXf"
   },
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1609925534801,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "3nAgTvyxCBXf",
    "outputId": "807a2fb0-d30f-4393-dd05-23238de4a20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Default seed set: 1234 ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI8gFOybCBXg"
   },
   "source": [
    "## Bible Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1609925536721,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "60KwZE5lCBXg"
   },
   "outputs": [],
   "source": [
    "# bible_english = open('./fr-en/king-james-bible.txt', encoding='utf-8').read().split('\\n')\n",
    "# fr_data = json.load(codecs.open('./fr-en/french_bible.json', 'r', 'utf-8-sig'))\n",
    "\n",
    "# bible_english = bible_english[:-1]\n",
    "# french_bible = []\n",
    "# fr_books_name = fr_data['bible']['booknames'].split(\",\")\n",
    "\n",
    "# for books_idx, books in enumerate(fr_data['bible']['b']):\n",
    "#     en_book_name = books['_n']\n",
    "#     fr_book_name = fr_books_name[books_idx]\n",
    "#     for chapter in books['c'] if isinstance(books['c'], (list)) else [books['c']]:\n",
    "#         for verse in chapter['v']:\n",
    "#             verse_ = en_book_name + f\" {chapter['_n']}:{verse['_n']} \\t {verse.get('__text', ' ')} \\t {fr_book_name} {chapter['_n']}:{verse['_n']}\"\n",
    "#             french_bible.append(verse_)\n",
    "\n",
    "# raw_data = {\"eng\": [ french_bible[index].split('\\t')[0] + ' ' + ' '.join([str(elem) for elem in line.split(' ')[2:]]) for index,line in enumerate(bible_english)],\n",
    "#            \"fre\": [line.split('\\t')[-1] + ' ' + line.split('\\t')[1] for index, line in enumerate(french_bible)]}\n",
    "\n",
    "# data_df = pd.DataFrame(raw_data, columns=['eng','fre'])\n",
    "# data_df.to_csv('./dataset/eng-fre-bible.csv', index=False)\n",
    "# data_df.to_json('./dataset/eng-fre-bible.json', orient='records', lines=True)\n",
    "\n",
    "# _, test_data = train_test_split(data_df, test_size=0.1)\n",
    "# _, valid_data = train_test_split(data_df, test_size=0.1)\n",
    "# test_data.to_json('./dataset/eng-fre-bible-test.json', orient='records', lines=True)\n",
    "# test_data.to_json('./dataset/eng-fre-bible-valid.json', orient='records', lines=True)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 43077,
     "status": "ok",
     "timestamp": 1609925622909,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "XzgMoZdQCBXh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real-music/anaconda3/envs/ray/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/real-music/anaconda3/envs/ray/lib/python3.7/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SOURCE = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "TARGET = Field(tokenize=tokenize_fr, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "\n",
    "fields = {'eng': ('src', SOURCE), 'fre': ('trg', TARGET)}\n",
    "\n",
    "train_data, test_data, valid_data = TabularDataset.splits(\n",
    "    path='./final_data/',\n",
    "    train='eng_fre_train.json',\n",
    "    test='eng_fre_test.json',\n",
    "    validation='eng_fre_valid.json',\n",
    "    format='json',\n",
    "    fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42164,
     "status": "ok",
     "timestamp": 1609878536610,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "jK_yCJDXCBXi",
    "outputId": "f41e6639-cdbd-46ec-8690-c292a3a6995a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['in', 'addition', ',', 'pray', 'in', 'the', 'spirit', 'often', ';', 'pray', 'in', 'tongues', '.', 'this', 'helps', 'to', 'condition', 'your', 'spirit', 'to', 'discern', 'god', '’s', 'voice', ',', 'visions', 'and', 'promptings', '.'], 'trg': ['de', 'plus', ',', 'priez', 'souvent', 'en', 'esprit', ';', 'priez', 'en', 'langues', '.', 'cela', 'aide', 'à', 'conditionner', 'votre', 'esprit', 'pour', 'discerner', 'la', 'voix', ',', 'les', 'visions', 'et', 'les', 'signaux', 'de', 'dieu', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[32000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42156,
     "status": "ok",
     "timestamp": 1609878536611,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "VlssBbtACBXj",
    "outputId": "fab72395-8a12-4565-c9ec-344b57ac956b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['i', 'have', 'not', 'spoken', 'to', 'him', 'today', 'and', 'i', 'will', 'not', 'do', 'so', 'in', 'future', '.'], 'trg': ['je', 'ne', 'lui', 'ai', 'jamais', 'adressé', 'la', 'parole', \"jusqu'\", 'ici', 'et', 'je', 'ne', 'le', 'ferai', 'pas', 'à', \"l'\", 'avenir', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[40000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2342,
     "status": "ok",
     "timestamp": 1609925625267,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "6RAUNGXJCBXj"
   },
   "outputs": [],
   "source": [
    "SOURCE.build_vocab(train_data, min_freq=1)\n",
    "TARGET.build_vocab(train_data, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2332,
     "status": "ok",
     "timestamp": 1609925625268,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "9au3g3I8CBXj",
    "outputId": "e6dc09fe-ced8-4746-f905-0cd1ed26b889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 13.7 gigabytes of available RAM\n",
      "\n",
      "To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n",
      "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
      "re-execute this cell.\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9CTFOCToCBXk"
   },
   "outputs": [],
   "source": [
    "# Read text file\n",
    "# english_txt = open('./fr-en/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "# french_txt = open('./fr-en/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "# print_terminal(f\"We have about {len(english_txt):,} sentence in out dataset.\")\n",
    "# raw_data = {'English': [line for line in english_txt],\n",
    "#            \"French\": [line for line in french_txt]}\n",
    "\n",
    "# # Convert to dataframe\n",
    "# data_df = pd.DataFrame(raw_data, columns=['English', 'French'])\n",
    "# data_df.head()\n",
    "\n",
    "# # Split data 50% training, 25% testing and 25% validation\n",
    "# train_data, test_data = train_test_split(data_df, test_size=0.25)\n",
    "# train_data, valid_data = train_test_split(train_data, test_size=0.25)\n",
    "\n",
    "# print_terminal(f\"Train size: {len(train_data):,}\")\n",
    "# print_terminal(f\"Test size: {len(test_data):,}\")\n",
    "# print_terminal(f\"Valid size: {len(valid_data):,}\")\n",
    "\n",
    "# dump csv data\n",
    "# train_data.to_csv('./dataset/train.csv', index=False)\n",
    "# test_data.to_csv('./dataset/test.csv', index=False)\n",
    "# valid_data.to_csv('./dataset/valid.csv', index=False)\n",
    "# print_terminal(\"Done writing [CVS]\")\n",
    "\n",
    "# dump csv json\n",
    "# train_data.to_json('./dataset/train.json', orient='records', lines=True)\n",
    "# test_data.to_json('./dataset/test.json', orient='records', lines=True)\n",
    "# valid_data.to_json('./dataset/valid.json', orient='records', lines=True)\n",
    "# print_terminal(\"Done writing [JSON]\")\n",
    "\n",
    "# SOURCE = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "# TARGET = Field(tokenize=tokenize_fr, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "\n",
    "# fields = {'English': ('src', SOURCE), 'French': ('trg', TARGET)}\n",
    "\n",
    "# train_data, test_data, valid_data = TabularDataset.splits(\n",
    "#     path='./dataset/',\n",
    "#     train='train.json',\n",
    "#     test='test.json',\n",
    "#     validation='valid.json',\n",
    "#     format='json',\n",
    "#     fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lYYYTrySCBXk"
   },
   "outputs": [],
   "source": [
    "# print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A_WHyaL3CBXl"
   },
   "outputs": [],
   "source": [
    "# SOURCE.build_vocab(train_data, min_freq=2)\n",
    "# TARGET.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "el-sUwbkCBXl"
   },
   "outputs": [],
   "source": [
    "# train_iterator, test_iterator, valid_iterator = BucketIterator.splits(\n",
    "#     (train_data, test_data, valid_data),\n",
    "#     batch_size=1000,\n",
    "#     shuffle=True,\n",
    "#     sort_key=lambda x: len(x.src),\n",
    "#     sort_within_batch=True,\n",
    "#     device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Wg4f_FAACBXl"
   },
   "outputs": [],
   "source": [
    "# for batch in train_iterator:\n",
    "#     print(batch.src)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDcleB_sCBXm"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2329,
     "status": "ok",
     "timestamp": 1609925625269,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "tMUxE4lKCBXm"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 embedding_size, \n",
    "                 num_layers, \n",
    "                 num_heads, \n",
    "                 forward_expansion,\n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_length = 100):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(max_length, embedding_size)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_size, \n",
    "                                                  num_heads, \n",
    "                                                  forward_expansion,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_size])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch_size, src_len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "#         print_terminal(src.shape)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src = [batch size, src len, embedding_size]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            \n",
    "        #src = [batch size, src len, embedding_size]\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t85iCklgCBXn"
   },
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1609925625270,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "zUPibvThCBXn"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, forward_expansion, dropout, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.ff_layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.self_attention = MultiHeadAttentionLayer(embedding_size, num_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(embedding_size, forward_expansion, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, embedding_size]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "        \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src = [batch size, src len, embedding_size]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src = [batch size, src len, embedding_size]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4lYYQWbCBXn"
   },
   "source": [
    "## Mutli Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 2324,
     "status": "ok",
     "timestamp": 1609925625270,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "FD7pDnYaCBXn"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert embedding_size % num_heads == 0\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_size // num_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc_k = nn.Linear(embedding_size, embedding_size)\n",
    "        self.fc_v = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "        self.fc_o = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.embedding_size)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYxRh2WQCBXo"
   },
   "source": [
    "## Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2322,
     "status": "ok",
     "timestamp": 1609925625271,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "GyTJKzYvCBXo"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, embedding_size, forward_expansion, dropout):\n",
    "        super(PositionwiseFeedforwardLayer, self).__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(embedding_size, forward_expansion)\n",
    "        self.fc_2 = nn.Linear(forward_expansion, embedding_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0H9VvWDpCBXo"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2320,
     "status": "ok",
     "timestamp": 1609925625272,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "eNUbiwELCBXo"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim, \n",
    "                 embedding_size, \n",
    "                 num_layers, \n",
    "                 num_heads, \n",
    "                 forward_expansion,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(max_length, embedding_size)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_size, \n",
    "                                                  num_heads, \n",
    "                                                  forward_expansion, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_size])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0 , trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "              \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJXfdsZWCBXo"
   },
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 2317,
     "status": "ok",
     "timestamp": 1609925625272,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "6N_wIOURCBXp"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_size, \n",
    "                 num_heads, \n",
    "                 forward_expansion, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.ff_layer_norm = nn.LayerNorm(embedding_size)\n",
    "        self.self_attention = MultiHeadAttentionLayer(embedding_size, num_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(embedding_size, num_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(embedding_size, \n",
    "                                                                     forward_expansion, \n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElDSoh_ACBXp"
   },
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2315,
     "status": "ok",
     "timestamp": 1609925625273,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "jRRH7jFyCBXp"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNn8im6XCBXp"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 1276,
     "status": "ok",
     "timestamp": 1609926617114,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "bzZkkL1TCBXq"
   },
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple(\"Run\", params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs\n",
    "    \n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_start_time = 0\n",
    "        \n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.valid_loss = 0\n",
    "        self.valid_iterator = None\n",
    "        \n",
    "        self.test_loss = 0\n",
    "        self.test_iterator = None\n",
    "        \n",
    "        self.best_params = OrderedDict()\n",
    "        \n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.iterator = None\n",
    "    \n",
    "    def begin_run(self, run, model, optimizer, iterator, valid_iterator, test_iterator):\n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.iterator = iterator\n",
    "        self.valid_iterator = valid_iterator\n",
    "        self.test_iterator = test_iterator\n",
    "        \n",
    "    def end_run(self):\n",
    "        self.epoch_count = 0\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.valid_loss = 0\n",
    "        self.test_loss = 0\n",
    "\n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "        \n",
    "        loss = self.epoch_loss / len(self.iterator)\n",
    "        valid_loss = self.valid_loss / len(self.valid_iterator)\n",
    "        test_loss = self.test_loss / len(self.test_iterator)\n",
    "        \n",
    "        results = OrderedDict()\n",
    "        results['run'] = self.run_count\n",
    "        results['epoch'] = self.epoch_count\n",
    "        \n",
    "        results['train loss'] = loss\n",
    "        results['valid loss'] = valid_loss\n",
    "        results['test loss'] = test_loss\n",
    "        \n",
    "        results['train perplexity'] = f'{math.exp(loss)}'\n",
    "        results['valid perplexity'] = f'{math.exp(valid_loss)}'\n",
    "        results['test perplexity'] = f'{math.exp(test_loss)}'\n",
    "        \n",
    "        results['epoch duration'] = epoch_duration\n",
    "        results['run duration'] = run_duration\n",
    "        \n",
    "        for k, v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "        \n",
    "    def track_loss(self, loss):\n",
    "        self.epoch_loss += loss.item()\n",
    "        \n",
    "    def track_valid_loss(self, loss):\n",
    "        self.valid_loss += loss.item()\n",
    "        \n",
    "    def track_test_loss(self, loss):\n",
    "        self.test_loss += loss.item()\n",
    "        \n",
    "    def save(self, fileName):\n",
    "        pd.DataFrame.from_dict(\n",
    "        self.run_data, \n",
    "        orient='columns').to_csv(f'./drive/MyDrive/Colab Notebooks/saved_result/{fileName}.csv')\n",
    "        \n",
    "        with open(f'./drive/MyDrive/Colab Notebooks/saved_result/{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "    def get_best_params(self, parameters: OrderedDict, num_to_display=5):\n",
    "\n",
    "        df = pd.read_csv('./drive/MyDrive/Colab Notebooks/results.csv')\n",
    "        df = df.drop(columns='Unnamed: 0', axis=1)\n",
    "        df = df.sort_values(['train loss', 'train perplexity'])\n",
    "        \n",
    "        new_params = df.head(1).to_dict('records')\n",
    "        for idx , key in enumerate(parameters):\n",
    "            \n",
    "            if key in new_params[0]:\n",
    "                parameters[key] = [new_params[0][key]]\n",
    "                \n",
    "        self.best_params = parameters\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print_terminal(f'Best params: {self.best_params}')\n",
    "        display(df.head(num_to_display))\n",
    "        \n",
    "    def train_with_best_params(self):\n",
    "        for run in RunBuilder.get_runs(self.best_params):\n",
    "            print_terminal(f\"Training with best params: {self.best_params}\")\n",
    "            \n",
    "            encoder = Encoder(src_vocab_size, embedding_size, num_encoder_layers, num_heads, forward_expansion, dropout, device)\n",
    "            decoder = Decoder(trg_vocab_size, embedding_size, num_decoder_layers, num_heads, forward_expansion, dropout, device)\n",
    "\n",
    "            model = Seq2Seq(encoder, decoder, src_pad_idx, trg_pad_idx, device).to(device)\n",
    "            model.apply(initialize_weights)\n",
    "\n",
    "            train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "                (train_data, valid_data, test_data),\n",
    "                batch_size=run.batch_size,\n",
    "                shuffle=run.shuffle,\n",
    "                sort_within_batch=True,\n",
    "                sort_key=lambda x: len(x.src),\n",
    "                device = device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=run.lr)\n",
    "            criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "            \n",
    "            best_valid_loss = float('inf')\n",
    "            \n",
    "            self.begin_run(run, model, optimizer, train_iterator, valid_iterator, test_iterator)\n",
    "            for epoch in range(num_epochs):\n",
    "                self.begin_epoch()\n",
    "\n",
    "                self.train_model(train_iterator, model, optimizer, criterion, clip, m)\n",
    "\n",
    "                self.end_epoch()\n",
    "            self.end_run()\n",
    "        self.save_checkpoint(\"seq2seq-bible\")\n",
    "        \n",
    "    \n",
    "    def save_checkpoint(self, filename='checkpoint'):\n",
    "        print_terminal(\"=> Saving checkpoint\")\n",
    "        checkpoint = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, f'./drive/MyDrive/Colab Notebooks/checkpoints/{filename}.pt')\n",
    "        print_terminal(f\"=> Done saving. Filename: {filename}\")\n",
    "\n",
    "    def load_checkpoint(self, model, device, optimizer, filename='checkpoint'):\n",
    "        print_terminal(\"=> Loading checkpoint\")\n",
    "        \n",
    "        checkpoint = torch.load(f\"./checkpoints/{filename}.pt\", map_location=device)\n",
    "        if model: model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        else: self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        \n",
    "        if optimizer: optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        else: self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        \n",
    "        print_terminal(f\"=> Done loading {filename}\")\n",
    "        return model if model else self.model, optimizer if optimizer else self.optimizer\n",
    "            \n",
    "    @staticmethod\n",
    "    def evaluate_model(iterator, model, criterion, m, test=False):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "\n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "                #output = [batch size, trg len - 1, output dim]\n",
    "                #trg = [batch size, trg len]\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output.contiguous().view(-1, output_dim)\n",
    "                trg = trg[:,1:].contiguous().view(-1)\n",
    "                #output = [batch size * trg len - 1, output dim]\n",
    "                #trg = [batch size * trg len - 1]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "                if test:\n",
    "                    m.track_valid_loss(loss) # valid_loss += loss.item()\n",
    "                m.track_test_loss(loss) # valid_loss += loss.item()\n",
    "            \n",
    "    @staticmethod\n",
    "    def train_model(iterator, model, optimizer, criterion, clip, m):\n",
    "        model.train()\n",
    "        for batch in iterator:\n",
    "\n",
    "            src = batch.src.to(device)\n",
    "            trg = batch.trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, trg) # calculation gradient\n",
    "            loss.backward() # Update weights\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            m.track_loss(loss) # epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8TtEl5DCBXq"
   },
   "source": [
    "## Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1609926621417,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "yDek33QeCBXq",
    "outputId": "9e06be73-051d-42bf-fc81-5289037c2cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup the training phase\n",
    "device = torch.device('cpu')\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0005\n",
    "batch_size = 128\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(SOURCE.vocab)\n",
    "trg_vocab_size = len(TARGET.vocab)\n",
    "embedding_size = 256\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "num_heads = 8\n",
    "forward_expansion = 512\n",
    "dropout = 0.1\n",
    "max_len = 500\n",
    "src_pad_idx = SOURCE.vocab.stoi[SOURCE.pad_token]\n",
    "trg_pad_idx = TARGET.vocab.stoi[TARGET.pad_token]\n",
    "clip = 1\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1609926625277,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "zAvDxFoaXvkx"
   },
   "outputs": [],
   "source": [
    "m = RunManager()\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "#   print('and then re-execute this cell.')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m_t9AKlCBXr"
   },
   "source": [
    "## Initialize Training with Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1609910175180,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "dvEAQ2B4CBXr",
    "outputId": "740b7709-ee25-4e60-a9cb-2e11a5dad9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== -Run(lr=0.0005, batch_size=32, shuffle=True, epochs=50) ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = OrderedDict(\n",
    "    lr = [0.0005],\n",
    "    batch_size = [32],\n",
    "    shuffle = [True],\n",
    "    epochs = [50],\n",
    ")\n",
    "\n",
    "\n",
    "for run in RunBuilder.get_runs(parameters):\n",
    "    print_terminal(f\"-{run}\")\n",
    "\n",
    "    encoder = Encoder(src_vocab_size, embedding_size, num_encoder_layers, num_heads, forward_expansion, dropout, device, max_len)\n",
    "    decoder = Decoder(trg_vocab_size, embedding_size, num_decoder_layers, num_heads, forward_expansion, dropout, device, max_len)\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder, src_pad_idx, trg_pad_idx, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size=run.batch_size,\n",
    "        shuffle=run.shuffle,\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device = device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=run.lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    m.begin_run(run, model, optimizer, train_iterator, valid_iterator, test_iterator)\n",
    "    for epoch in range(run.epochs):\n",
    "        m.begin_epoch()\n",
    "\n",
    "        m.train_model(train_iterator, model, optimizer, criterion, clip, m)\n",
    "        # m.evaluate_model(valid_iterator, model, criterion, m)\n",
    "        # m.evaluate_model(test_iterator, model, criterion, m)\n",
    "        \n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('eng-fre-results')\n",
    "m.save_checkpoint(\"eng-fre-checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5hcxK9uCBXr"
   },
   "source": [
    "## Best Hyperparams with low loss and low perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 6230861,
     "status": "ok",
     "timestamp": 1609884725417,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "m8pJvoO5CBXs",
    "outputId": "eb97617b-a4e8-44b3-db13-a3878d857147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Best params: OrderedDict([('lr', [0.0005]), ('batch_size', [32]), ('shuffle', [True]), ('epochs', [150])]) ==========\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train loss</th>\n",
       "      <th>valid loss</th>\n",
       "      <th>test loss</th>\n",
       "      <th>train perplexity</th>\n",
       "      <th>valid perplexity</th>\n",
       "      <th>test perplexity</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.693095</td>\n",
       "      <td>9927.639762</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.079367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.189756</td>\n",
       "      <td>9993.864984</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.083837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.172149</td>\n",
       "      <td>9727.948566</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>0.081053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.084429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.527576</td>\n",
       "      <td>9529.510191</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>0.081179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.084564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.132950</td>\n",
       "      <td>9794.119493</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     run  epoch  train loss  valid loss  ...      lr  batch_size  shuffle  epochs\n",
       "148    1    149    0.079366         0.0  ...  0.0005          32     True     150\n",
       "149    1    150    0.079367         0.0  ...  0.0005          32     True     150\n",
       "145    1    146    0.080508         0.0  ...  0.0005          32     True     150\n",
       "142    1    143    0.081053         0.0  ...  0.0005          32     True     150\n",
       "146    1    147    0.081179         0.0  ...  0.0005          32     True     150\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.get_best_params(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy9C_cziCBXs"
   },
   "source": [
    "## Save model with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T92qOg1YCBXs"
   },
   "outputs": [],
   "source": [
    "# m.train_with_best_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDneYdEPCBXs"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 898,
     "status": "error",
     "timestamp": 1609926629842,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "mAyzDH-GCBXs",
    "outputId": "52262745-fe3d-46d9-c04d-01c07baf50aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== => Loading checkpoint ==========\n",
      "\n",
      "========== => Done loading eng-fre-checkpoint ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(src_vocab_size, embedding_size, num_encoder_layers, num_heads, forward_expansion, dropout, device, max_len)\n",
    "decoder = Decoder(trg_vocab_size, embedding_size, num_decoder_layers, num_heads, forward_expansion, dropout, device, max_len)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, src_pad_idx, trg_pad_idx, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "model,_ = m.load_checkpoint(filename=\"eng-fre-checkpoint\",model=model, device=device, optimizer=optimizer)\n",
    "\n",
    "# valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "# print(f'| Valid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0QhmC1JCBXt"
   },
   "source": [
    "## Model BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVjPFz5FCBXt"
   },
   "outputs": [],
   "source": [
    "blue_score = calculate_bleu(test_data, SOURCE, TARGET, model, device, max_len)\n",
    "print_terminal(f'BLEU score = {blue_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEzCgs6zCBXt"
   },
   "source": [
    "## Example From Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 949,
     "status": "error",
     "timestamp": 1609925399519,
     "user": {
      "displayName": "Real Music",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiMqwYb-NSA4wy0JuypbWI9OrCl4W3iFh7Ba-YlNw=s64",
      "userId": "18220127434386320225"
     },
     "user_tz": -60
    },
    "id": "2lEzCgc8CBXt",
    "outputId": "a2cdf7a4-b1b3-4521-d3c4-44d867020916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== src = 2 kings 21:11   because manasseh king of judah hath done these abominations , [ and ] hath done wickedly above all that the amorites did , which [ were ] before him , and hath made judah also to sin with his idols : ==========\n",
      "\n",
      "========== trg =   2 rois 21:11   parce que manassé , roi de juda , a pratiqué ces abominations , et a fait le mal plus que tout ce qu' ont fait les amoréens qui ont été avant lui , et qu' il a fait pécher aussi juda par ses idoles , ==========\n",
      "\n",
      "========== predicted trg =   2 rois 21:11   parce que manassé , roi de juda , a pratiqué ces abominations , et a fait le mal plus que tout ce que les amoréens qui ont fait auparavant , et a été avant lui , et a fait pécher aussi juda avec ses idoles , <eos> ==========\n",
      "\n",
      "========== 52 ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_test(train_data, SOURCE, TARGET, model, device, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmaUQf28CBXu"
   },
   "source": [
    "## Example From Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "wFHCPHb1CBXu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== src = we must create awareness in this area instead of further devaluing it with subsistence wages . ==========\n",
      "\n",
      "========== trg = nous devons sensibiliser l’ opinion publique à cet égard , au lieu de continuer à dévaloriser ce travail avec des salaires de subsistance . ==========\n",
      "\n",
      "========== predicted trg = il faut créer de quelque chose de plus de profit pour le savoirfaire acquis par an avec les revenus . <eos> ==========\n",
      "\n",
      "========== 21 ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_test(valid_data, SOURCE, TARGET, model, device, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boScxKKwCBXu"
   },
   "source": [
    "## Example From Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0UYVbJreCBXv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== src = this morning , in accordance with the rules of procedure , we must elect the president . ==========\n",
      "\n",
      "========== trg = ce matin , en vertu du règlement , nous devons élire le président . ==========\n",
      "\n",
      "========== predicted trg = cette procédure doit conformément aux règles de procédure , nous devons élus . <eos> ==========\n",
      "\n",
      "========== 14 ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_test(test_data, SOURCE, TARGET, model, device, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== la défense de la justice , vous êtes vraiment intéressée sur le plan européen et les adversités . l’ adversité , est soumis à la télématique établit , et les éléments dont je dispose et que son règne et que vous marchiez . <eos> ==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Without an understanding of righteousness, you \\\n",
    "really can’t live victoriously over the adversary and \\\n",
    "adversities. You’ll be subject to the elements of this\\\n",
    "world, whereas you’re meant to rule and reign over\\\n",
    "them.'\n",
    "sen, att = translate_sentence(sentence, SOURCE, TARGET, model, device, max_len)\n",
    "sent = ' '.join([a for a in sen])\n",
    "print_terminal(f'{sent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9EDXCafECBXa",
    "ZnTfwZOGCBXc",
    "K6wx9Z_xCBXf",
    "BDcleB_sCBXm",
    "t85iCklgCBXn",
    "R4lYYQWbCBXn",
    "RYxRh2WQCBXo",
    "0H9VvWDpCBXo",
    "pJXfdsZWCBXo",
    "ElDSoh_ACBXp",
    "W0QhmC1JCBXt",
    "BmaUQf28CBXu",
    "boScxKKwCBXu"
   ],
   "name": "seq2seq_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

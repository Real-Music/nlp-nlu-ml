{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Chatbot\n",
    "1. Tuto link https://medium.com/predict/creating-a-chatbot-from-scratch-using-keras-and-tensorflow-59e8fc76be79\n",
    "2. Download dataset https://www.kaggle.com/kausr25/chatterbotenglish#botprofile.yml    \n",
    "3. Learn https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "4. Padding https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "5. Onehot encoding https://stackoverflow.com/questions/41494625/issues-using-keras-np-utils-to-categorical/53430549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/real-music/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mute_tf_warnings import tf_mute_warning\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "tf_mute_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data_seq2seq/'\n",
    "files_list = os.listdir(dir_path + os.sep) # our dateset in a list\n",
    "files_list.remove('.ipynb_checkpoints')\n",
    "files_list.remove('politics.yml')\n",
    "files_list.remove('gossip.yml')\n",
    "files_list.remove('history.yml')\n",
    "files_list.remove('movies.yml')\n",
    "files_list.remove('money.yml')\n",
    "files_list.remove('trivia.yml')\n",
    "files_list.remove('sports.yml')\n",
    "files_list.remove('psychology.yml')\n",
    "files_list.remove('literature.yml')\n",
    "\n",
    "\n",
    "questions = []\n",
    "answers = list()\n",
    "for filepath in files_list:\n",
    "    stream = open(dir_path + os.sep + filepath, 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "\n",
    "    conversations = docs['conversations']\n",
    "    \n",
    "    for con in conversations:\n",
    "        if len(con) > 2:\n",
    "            questions.append(con[0])\n",
    "            replies = con[1:]\n",
    "            ans = ''\n",
    "            \n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "            \n",
    "        elif len(con) > 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My brain does not require any beverages.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>My brain does not require any beverages.<END>\n"
     ]
    }
   ],
   "source": [
    "answers_with_tags = list()\n",
    "for i in range(len(answers)):\n",
    "    if type( answers[i] ).__name__ == 'str':\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ):\n",
    "    answers.append('<START>' + answers_with_tags[i] + '<END>' )\n",
    "\n",
    "    \n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186\n"
     ]
    }
   ],
   "source": [
    "print(len( tokenizer.word_index )+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "love = 0\n",
    "for word in questions:\n",
    "#     word = word.lower()\n",
    "    if word.find('What') != -1:\n",
    "        love += 1\n",
    "print(love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "hey = 'I love you'\n",
    "print(hey.find('me'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for Seq2Seq model\n",
    "Our model requires three arrays namely encoder_input_data, decoder_input_data and decoder_output_data.\n",
    "\n",
    "1. For encoder_input_data :\n",
    "    * Tokenize the questions. Pad them to their maximum length.\n",
    "2. For decoder_input_data :\n",
    "    * Tokenize the answers. Pad them to their maximum length.\n",
    "3. For decoder_output_data :\n",
    "    * Tokenize the answers. Remove the first element from all the tokenized_answers. This is the START> element which we added earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 9)\n",
      "(303, 74)\n",
      "(303, 74, 1186)\n"
     ]
    }
   ],
   "source": [
    "# # encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print( encoder_input_data.shape)\n",
    "\n",
    "# # decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( len(x) for x in tokenized_answers )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers, maxlen=maxlen_answers, padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers ) # here we are removing the <start> sequence\n",
    "for i in range(len(tokenized_answers)):\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
    "onehot_answers = utils.to_categorical( padded_answers, VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape)\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "np.save('./saved arrays/enc_in_data.npy', encoder_input_data)\n",
    "np.save('./saved arrays/dec_in_data.npy', decoder_input_data)\n",
    "np.save('./saved arrays/dec_tar_data.npy', decoder_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Defining the Encoder-Decoder model\n",
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n",
    "\n",
    "**Important points :**\n",
    "\n",
    "\n",
    "*   `200` is the output of the GloVe embeddings.\n",
    "*   `embedding_matrix` is the GloVe embedding which we downloaded earlier.\n",
    "\n",
    "\n",
    "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
    "\n",
    "\n",
    "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    237200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    237200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1186)   238386      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,354,386\n",
      "Trainable params: 1,354,386\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM( 200, return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h, state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _,_ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax )\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training the model\n",
    "We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "303/303 [==============================] - 6s 21ms/sample - loss: 1.5740\n",
      "Epoch 2/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.3276\n",
      "Epoch 3/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 1.2886\n",
      "Epoch 4/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2769\n",
      "Epoch 5/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 1.2667\n",
      "Epoch 6/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2557\n",
      "Epoch 7/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2447\n",
      "Epoch 8/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2335\n",
      "Epoch 9/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2210\n",
      "Epoch 10/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.2099\n",
      "Epoch 11/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 1.1968\n",
      "Epoch 12/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1846\n",
      "Epoch 13/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1724\n",
      "Epoch 14/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1585\n",
      "Epoch 15/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1452\n",
      "Epoch 16/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1310\n",
      "Epoch 17/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1167\n",
      "Epoch 18/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.1023\n",
      "Epoch 19/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.0868\n",
      "Epoch 20/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.0732\n",
      "Epoch 21/150\n",
      "303/303 [==============================] - 3s 12ms/sample - loss: 1.0602\n",
      "Epoch 22/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.0453\n",
      "Epoch 23/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.0332\n",
      "Epoch 24/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 1.0189\n",
      "Epoch 25/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 1.0050\n",
      "Epoch 26/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9929\n",
      "Epoch 27/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9807\n",
      "Epoch 28/150\n",
      "303/303 [==============================] - 4s 14ms/sample - loss: 0.9678\n",
      "Epoch 29/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9539\n",
      "Epoch 30/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.9416\n",
      "Epoch 31/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9301\n",
      "Epoch 32/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9184\n",
      "Epoch 33/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.9117\n",
      "Epoch 34/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8947\n",
      "Epoch 35/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8832\n",
      "Epoch 36/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.8741\n",
      "Epoch 37/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.8606\n",
      "Epoch 38/150\n",
      "303/303 [==============================] - 3s 12ms/sample - loss: 0.8545\n",
      "Epoch 39/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8395\n",
      "Epoch 40/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.8278\n",
      "Epoch 41/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8192\n",
      "Epoch 42/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8067\n",
      "Epoch 43/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.8002\n",
      "Epoch 44/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.7876\n",
      "Epoch 45/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.7784\n",
      "Epoch 46/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7682\n",
      "Epoch 47/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7579\n",
      "Epoch 48/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7523\n",
      "Epoch 49/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7406\n",
      "Epoch 50/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7282\n",
      "Epoch 51/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7213\n",
      "Epoch 52/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.7114\n",
      "Epoch 53/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.7006\n",
      "Epoch 54/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.6938\n",
      "Epoch 55/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6846\n",
      "Epoch 56/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6732\n",
      "Epoch 57/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.6674\n",
      "Epoch 58/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6545\n",
      "Epoch 59/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6472\n",
      "Epoch 60/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6373\n",
      "Epoch 61/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6323\n",
      "Epoch 62/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.6200\n",
      "Epoch 63/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.6100\n",
      "Epoch 64/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5995\n",
      "Epoch 65/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.5911\n",
      "Epoch 66/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.5850\n",
      "Epoch 67/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5740\n",
      "Epoch 68/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5640\n",
      "Epoch 69/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5552\n",
      "Epoch 70/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5473\n",
      "Epoch 71/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.5372\n",
      "Epoch 72/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5288\n",
      "Epoch 73/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5205\n",
      "Epoch 74/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5125\n",
      "Epoch 75/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.5050\n",
      "Epoch 76/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4959\n",
      "Epoch 77/150\n",
      "303/303 [==============================] - 4s 13ms/sample - loss: 0.4887\n",
      "Epoch 78/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4798\n",
      "Epoch 79/150\n",
      "303/303 [==============================] - 3s 12ms/sample - loss: 0.4743\n",
      "Epoch 80/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.4638\n",
      "Epoch 81/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4586\n",
      "Epoch 82/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4506\n",
      "Epoch 83/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4398\n",
      "Epoch 84/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.4368\n",
      "Epoch 85/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4274\n",
      "Epoch 86/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4176\n",
      "Epoch 87/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4137\n",
      "Epoch 88/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.4147\n",
      "Epoch 89/150\n",
      "303/303 [==============================] - 4s 12ms/sample - loss: 0.3981\n",
      "Epoch 90/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.3888\n",
      "Epoch 91/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3840\n",
      "Epoch 92/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3796\n",
      "Epoch 93/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3713\n",
      "Epoch 94/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3661\n",
      "Epoch 95/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3612\n",
      "Epoch 96/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3519\n",
      "Epoch 97/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3447\n",
      "Epoch 98/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3384\n",
      "Epoch 99/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3334\n",
      "Epoch 100/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3299\n",
      "Epoch 101/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3216\n",
      "Epoch 102/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3196\n",
      "Epoch 103/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.3087\n",
      "Epoch 104/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.3026\n",
      "Epoch 105/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2979\n",
      "Epoch 106/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2965\n",
      "Epoch 107/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2901\n",
      "Epoch 108/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2824\n",
      "Epoch 109/150\n",
      "303/303 [==============================] - 3s 9ms/sample - loss: 0.2785\n",
      "Epoch 110/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2731\n",
      "Epoch 111/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2665\n",
      "Epoch 112/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2710\n",
      "Epoch 113/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2571\n",
      "Epoch 114/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2533\n",
      "Epoch 115/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2519\n",
      "Epoch 116/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2456\n",
      "Epoch 117/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2385\n",
      "Epoch 118/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2330\n",
      "Epoch 119/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2358\n",
      "Epoch 120/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2271\n",
      "Epoch 121/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2208\n",
      "Epoch 122/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2173\n",
      "Epoch 123/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2152\n",
      "Epoch 124/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2116\n",
      "Epoch 125/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.2057\n",
      "Epoch 126/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.2060\n",
      "Epoch 127/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1964\n",
      "Epoch 128/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1923\n",
      "Epoch 129/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1884\n",
      "Epoch 130/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.1861\n",
      "Epoch 131/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1862\n",
      "Epoch 132/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1768\n",
      "Epoch 133/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1756\n",
      "Epoch 134/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1706\n",
      "Epoch 135/150\n",
      "303/303 [==============================] - 3s 11ms/sample - loss: 0.1699\n",
      "Epoch 136/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1687\n",
      "Epoch 137/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1610\n",
      "Epoch 138/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1596\n",
      "Epoch 139/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1539\n",
      "Epoch 140/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1541\n",
      "Epoch 141/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1510\n",
      "Epoch 142/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1458\n",
      "Epoch 143/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1442\n",
      "Epoch 144/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1396\n",
      "Epoch 145/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1381\n",
      "Epoch 146/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1344\n",
      "Epoch 147/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1320\n",
      "Epoch 148/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1293\n",
      "Epoch 149/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1259\n",
      "Epoch 150/150\n",
      "303/303 [==============================] - 3s 10ms/sample - loss: 0.1246\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=50, epochs=150) #75\n",
    "model.save('./seq_saved_model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Defining inference models\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=(200, ))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=(200, ))\n",
    "    \n",
    "    decoder_states_inputs = [ decoder_state_input_h, decoder_state_input_c ]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [ state_h, state_c ]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Talking with our Chatbot\n",
    "\n",
    "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence: str):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ])\n",
    "    return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello end\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " greetings end\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  welcome\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'welcome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-05619eeb270a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mempty_target_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mempty_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-82fd3ff94dac>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokens_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtokens_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'welcome'"
     ]
    }
   ],
   "source": [
    "enc_model, dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens(input('You: ')))\n",
    "    empty_target_seq = np.zeros((1,1))\n",
    "    empty_target_seq[0,0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition:\n",
    "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0,-1,:])\n",
    "        sampled_word = None\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if sampled_word_index == index:\n",
    "                decoded_translation += f' {word}'\n",
    "                sampled_word = word\n",
    "                \n",
    "            if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "                stop_condition = True\n",
    "                \n",
    "            empty_target_seq = np.zeros((1,1))\n",
    "            empty_target_seq[0,0] = sampled_word_index\n",
    "            states_values = [ h, c ]\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
